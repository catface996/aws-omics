# 预处理工作流完整指南

## 📖 工作流概述

`preprocessing_workflow.wdl` 是一个完整的基因组测序数据预处理工作流，使用 WDL (Workflow Description Language) 编写，专门用于 AWS Omics 平台。该工作流实现了从原始测序数据到高质量预处理数据的完整流程。

### 🎯 主要功能
- 原始测序数据质量评估
- 接头去除和质量过滤
- 数据格式转换和标准化
- 重复序列去除
- 综合质量报告生成

---

## 🏗️ WDL 结构分析

### 1. 版本和导入声明

```wdl
version 1.0

import "tasks/fastqc_task.wdl" as FastQC
import "tasks/trimmomatic_task.wdl" as Trimmomatic
import "tasks/fastp_task.wdl" as FastP
import "tasks/deduplication_task.wdl" as Dedup
import "tasks/multiqc_task.wdl" as MultiQC
```

**功能说明：**
- 使用 WDL 1.0 语法标准
- 导入 5 个任务模块，每个负责特定的预处理步骤
- 模块化设计便于维护和重用

### 2. 元数据定义

```wdl
meta {
    description: "完整的测序数据预处理工作流..."
    version: "1.0"
    author: "AWS Omics Demo Project"
}

parameter_meta {
    sample_name: "样本名称"
    fastq_r1: "正向测序文件 (R1)"
    fastq_r2: "反向测序文件 (R2，可选)"
    reference_genome: "参考基因组文件"
    adapter_fasta: "接头序列文件"
    min_length: "最小读长阈值"
    min_quality: "最小质量分数"
    threads: "并行线程数"
}
```

**功能说明：**
- 提供工作流描述和版本信息
- 为每个输入参数提供中文说明
- 便于用户理解参数用途

### 3. 输入参数定义

```wdl
input {
    String sample_name                    # 样本名称
    File fastq_r1                        # 必需：R1 文件
    File? fastq_r2                       # 可选：R2 文件
    File? reference_genome               # 可选：参考基因组
    File? adapter_fasta                  # 可选：接头序列
    Int min_length = 50                  # 默认：最小读长
    Int min_quality = 20                 # 默认：最小质量
    Int threads = 8                      # 默认：线程数
    Boolean paired_end = defined(fastq_r2)  # 自动判断是否双端
}
```

**参数类型说明：**
- `String` - 字符串类型，用于样本名称
- `File` - 文件类型，必需的输入文件
- `File?` - 可选文件类型，带问号表示可选
- `Int` - 整数类型，用于数值参数
- `Boolean` - 布尔类型，自动判断处理模式

---

## 🔄 工作流执行步骤概览

### 数据流程图

```
原始 FASTQ 文件 (R1, R2)
         ↓
   初始质量评估 (FastQC)
    ├─ HTML 报告
    └─ 统计数据
         ↓
   接头去除和质量过滤 (Trimmomatic)
    ├─ 去除接头序列
    ├─ 质量分数过滤
    └─ 长度过滤
         ↓
   高级质量过滤 (fastp)
    ├─ 复杂度过滤
    ├─ PolyG/PolyX 修剪
    └─ 自动接头检测
         ↓
   去重复序列 (Deduplication)
    ├─ PCR 重复去除
    └─ 序列多样性保持
         ↓
   最终质量评估 (FastQC)
    ├─ 处理后质量报告
    └─ 质量改善验证
         ↓
   综合报告生成 (MultiQC)
    ├─ 整合所有报告
    ├─ 前后对比分析
    └─ 交互式展示
         ↓
   预处理完成的高质量数据
    ├─ 清洁的 FASTQ 文件
    ├─ 详细的质量报告
    └─ 处理统计信息
```

---
# 📋 工作流步骤详细参数解析

## 🔍 步骤 1：FastQC 质量评估

### 任务：`RunFastQC`

FastQC 是测序数据质量评估的标准工具，用于生成详细的质量控制报告。

#### 工作流调用

```wdl
call FastQC.RunFastQC as InitialQC_R1 {
    input:
        fastq_file = fastq_r1,
        sample_name = sample_name + "_R1_initial",
        threads = threads
}

if (paired_end) {
    call FastQC.RunFastQC as InitialQC_R2 {
        input:
            fastq_file = select_first([fastq_r2]),
            sample_name = sample_name + "_R2_initial",
            threads = threads
    }
}
```

#### 输入参数详解

| 参数名 | 类型 | 默认值 | 说明 | 用途 |
|--------|------|--------|------|------|
| `fastq_file` | File | 必需 | 输入的 FASTQ 文件 | 待分析的测序数据文件 |
| `sample_name` | String | 必需 | 样本名称 | 用于输出文件命名和报告标识 |
| `threads` | Int | 4 | 并行线程数 | 控制 FastQC 使用的 CPU 核心数 |
| `memory_gb` | Int | 8 | 内存需求(GB) | 分配给任务的内存大小 |
| `disk_gb` | Int | 50 | 磁盘空间需求(GB) | 临时文件和输出文件的存储空间 |

#### 参数优化建议

**`threads` (线程数)**
- **范围**: 1-16
- **推荐值**: 4-8
- **影响**: 
  - 更多线程可以加速大文件的处理
  - 对于小文件（<1GB），增加线程数效果有限
  - 过多线程可能导致内存竞争

**`memory_gb` (内存)**
- **范围**: 4-32 GB
- **推荐值**: 
  - 小文件（<5GB）: 8GB
  - 中等文件（5-20GB）: 16GB
  - 大文件（>20GB）: 32GB
- **影响**: 内存不足会导致任务失败或运行缓慢

#### 输出文件

| 文件名 | 格式 | 内容 | 用途 |
|--------|------|------|------|
| `*_fastqc.html` | HTML | 可视化质量报告 | 人工查看和质量评估 |
| `*_fastqc.zip` | ZIP | 详细统计数据 | MultiQC 整合和程序化分析 |
| `*_summary.txt` | TXT | 关键指标摘要 | 快速质量检查 |

#### 质量指标解读

**关键指标**:
- **总序列数**: 测序深度指标
- **序列长度**: 读长分布
- **GC含量**: 基因组特征和污染检测
- **质量分数分布**: 测序质量评估
- **重复序列水平**: PCR 重复和过度表示序列

---

## ✂️ 步骤 2：Trimmomatic 接头去除和质量过滤

### 任务：`RunTrimmomaticPE` / `RunTrimmomaticSE`

Trimmomatic 是广泛使用的测序数据预处理工具，用于去除接头序列和低质量区域。

#### 工作流调用

```wdl
if (paired_end) {
    call Trimmomatic.RunTrimmomaticPE as TrimPE {
        input:
            fastq_r1 = fastq_r1,
            fastq_r2 = select_first([fastq_r2]),
            sample_name = sample_name,
            adapter_fasta = adapter_fasta,
            min_length = min_length,
            min_quality = min_quality,
            threads = threads
    }
}

if (!paired_end) {
    call Trimmomatic.RunTrimmomaticSE as TrimSE {
        input:
            fastq = fastq_r1,
            sample_name = sample_name,
            adapter_fasta = adapter_fasta,
            min_length = min_length,
            min_quality = min_quality,
            threads = threads
    }
}
```

#### 输入参数详解

| 参数名 | 类型 | 默认值 | 说明 | 用途 |
|--------|------|--------|------|------|
| `fastq_r1` | File | 必需 | 正向测序文件 (R1) | 双端测序的第一个文件 |
| `fastq_r2` | File | 必需 | 反向测序文件 (R2) | 双端测序的第二个文件 |
| `sample_name` | String | 必需 | 样本名称 | 输出文件命名前缀 |
| `adapter_fasta` | File? | 可选 | 接头序列文件 | 自定义接头序列，默认使用 TruSeq3 |
| `min_length` | Int | 50 | 最小读长阈值 | 短于此长度的读段将被丢弃 |
| `min_quality` | Int | 20 | 最小质量分数 | 滑动窗口质量过滤的阈值 |
| `threads` | Int | 8 | 并行线程数 | CPU 核心数 |
| `memory_gb` | Int | 16 | 内存需求(GB) | 内存分配 |
| `disk_gb` | Int | 100 | 磁盘空间需求(GB) | 存储空间 |

#### 参数优化策略

**`min_length` (最小读长)**
- **范围**: 20-150 bp
- **推荐值**: 
  - Illumina HiSeq/NovaSeq: 50-75 bp
  - Illumina MiSeq: 30-50 bp
  - 长读长平台: 100+ bp
- **影响**: 
  - 过低: 保留低质量短片段，影响比对质量
  - 过高: 丢失有用数据，降低测序深度

**`min_quality` (最小质量分数)**
- **范围**: 15-30
- **推荐值**: 
  - 高质量数据: 25-30
  - 标准质量: 20-25
  - 低质量数据: 15-20
- **影响**: 
  - 过低: 保留低质量碱基，增加错误率
  - 过高: 过度修剪，丢失有用信息

#### Trimmomatic 处理参数详解

**接头去除参数 (ILLUMINACLIP)**
```bash
ILLUMINACLIP:adapter_file:2:30:10
```
- `2`: 种子匹配允许的错配数
- `30`: 回文匹配阈值（双端数据接头匹配）
- `10`: 简单匹配阈值（单端接头匹配）

**质量过滤参数**
```bash
LEADING:3          # 去除开头质量<3的碱基
TRAILING:3         # 去除结尾质量<3的碱基
SLIDINGWINDOW:4:20 # 滑动窗口(4bp)平均质量<20时修剪
MINLEN:50          # 最小长度过滤
```

#### 输出文件

| 文件名 | 内容 | 用途 |
|--------|------|------|
| `*_R1_paired.fastq.gz` | 配对的 R1 读段 | 主要输出，用于下游分析 |
| `*_R2_paired.fastq.gz` | 配对的 R2 读段 | 主要输出，用于下游分析 |
| `*_R1_unpaired.fastq.gz` | 未配对的 R1 读段 | 质量控制参考 |
| `*_R2_unpaired.fastq.gz` | 未配对的 R2 读段 | 质量控制参考 |
| `*_trimmomatic.log` | 处理日志 | 统计信息和错误诊断 |

---
## 🚀 步骤 3：FastP 高级质量过滤

### 任务：`RunFastPPE` / `RunFastPSE`

FastP 是新一代的测序数据预处理工具，提供比 Trimmomatic 更精细的控制和更快的处理速度。

#### 工作流调用

```wdl
if (paired_end) {
    call FastP.RunFastPPE as FastPPE {
        input:
            fastq_r1 = select_first([TrimPE.trimmed_r1]),
            fastq_r2 = select_first([TrimPE.trimmed_r2]),
            sample_name = sample_name,
            min_length = min_length,
            min_quality = min_quality,
            threads = threads
    }
}

if (!paired_end) {
    call FastP.RunFastPSE as FastPSE {
        input:
            fastq = select_first([TrimSE.trimmed]),
            sample_name = sample_name,
            min_length = min_length,
            min_quality = min_quality,
            threads = threads
    }
}
```

#### 输入参数详解

| 参数名 | 类型 | 默认值 | 说明 | 用途 |
|--------|------|--------|------|------|
| `fastq_r1` | File | 必需 | 正向测序文件 | 来自 Trimmomatic 的输出 |
| `fastq_r2` | File | 必需 | 反向测序文件 | 来自 Trimmomatic 的输出 |
| `sample_name` | String | 必需 | 样本名称 | 文件命名 |
| `min_length` | Int | 50 | 最小读长阈值 | 长度过滤 |
| `min_quality` | Int | 20 | 最小质量分数 | 质量过滤 |
| `threads` | Int | 8 | 并行线程数 | 性能控制 |
| `max_length` | Int | 500 | 最大读长阈值 | 去除异常长的读段 |
| `complexity_threshold` | Int | 30 | 复杂度阈值 | 低复杂度序列过滤 |
| `enable_polyg_trimming` | Boolean | true | 启用 PolyG 修剪 | 去除 NovaSeq 的 PolyG 尾部 |
| `enable_polyx_trimming` | Boolean | true | 启用 PolyX 修剪 | 去除同聚物尾部 |

#### 高级参数详解

**`max_length` (最大读长)**
- **范围**: 200-1000 bp
- **推荐值**: 
  - Illumina 150bp PE: 300-400 bp
  - Illumina 250bp PE: 500-600 bp
- **影响**: 去除异常长的读段，通常是接头二聚体或其他人工产物

**`complexity_threshold` (复杂度阈值)**
- **范围**: 10-50
- **推荐值**: 
  - 严格过滤: 40-50
  - 标准过滤: 30-40
  - 宽松过滤: 20-30
- **影响**: 
  - 过高: 可能去除真实的低复杂度区域（如重复序列）
  - 过低: 保留低质量的同聚物序列

**PolyG/PolyX 修剪**
- **PolyG**: NovaSeq 平台特有的问题，暗循环产生连续的 G
- **PolyX**: 任何同聚物尾部（AAAA, TTTT, CCCC, GGGG）
- **建议**: NovaSeq 数据必须启用，其他平台可选

#### FastP 处理选项

```bash
--unqualified_percent_limit 40    # 允许40%的碱基质量低于阈值
--n_base_limit 5                  # 允许最多5个N碱基
--low_complexity_filter           # 启用低复杂度过滤
--overrepresentation_analysis     # 过度表示序列分析
```

#### 输出文件

| 文件名 | 格式 | 内容 | 用途 |
|--------|------|------|------|
| `*_R1_filtered.fastq.gz` | FASTQ | 过滤后的 R1 数据 | 主要输出 |
| `*_R2_filtered.fastq.gz` | FASTQ | 过滤后的 R2 数据 | 主要输出 |
| `*_fastp.html` | HTML | 详细的处理报告 | 质量评估和可视化 |
| `*_fastp.json` | JSON | 机器可读的统计数据 | 程序化分析 |

#### 质量改善指标

FastP 报告中的关键指标：
- **读段保留率**: 处理后保留的读段百分比
- **碱基保留率**: 处理后保留的碱基百分比
- **Q30 改善**: 质量分数≥30的碱基比例变化
- **GC 含量稳定性**: 处理前后 GC 含量变化
- **长度分布**: 读段长度分布的变化

---

## 🔄 步骤 4：去重复序列处理

### 任务：`RemoveDuplicatesPE` / `RemoveDuplicatesSE`

去重复处理用于移除 PCR 重复和光学重复，提高数据质量和分析效率。

#### 工作流调用

```wdl
if (paired_end) {
    call Dedup.RemoveDuplicatesPE as DedupPE {
        input:
            fastq_r1 = select_first([FastPPE.filtered_r1]),
            fastq_r2 = select_first([FastPPE.filtered_r2]),
            sample_name = sample_name,
            threads = threads
    }
}

if (!paired_end) {
    call Dedup.RemoveDuplicatesSE as DedupSE {
        input:
            fastq = select_first([FastPSE.filtered]),
            sample_name = sample_name,
            threads = threads
    }
}
```

#### 输入参数详解

| 参数名 | 类型 | 默认值 | 说明 | 用途 |
|--------|------|--------|------|------|
| `fastq_r1` | File | 必需 | 过滤后的 R1 文件 | 来自 FastP 的输出 |
| `fastq_r2` | File | 必需 | 过滤后的 R2 文件 | 来自 FastP 的输出 |
| `sample_name` | String | 必需 | 样本名称 | 文件命名 |
| `threads` | Int | 8 | 并行线程数 | 性能控制 |
| `method` | String | "fastuniq" | 去重复方法 | 算法选择 |
| `memory_gb` | Int | 16 | 内存需求 | 资源分配 |

#### 去重复方法对比

**1. FastUniq**
- **原理**: 基于序列完全匹配
- **优点**: 
  - 专为双端数据设计
  - 保持配对关系
  - 处理速度快
- **缺点**: 
  - 只能检测完全相同的重复
  - 内存使用较高
- **适用**: 标准 Illumina 双端数据

**2. Clumpify (BBTools)**
- **原理**: 基于 k-mer 聚类
- **优点**: 
  - 可检测近似重复
  - 支持光学重复检测
  - 内存效率高
- **缺点**: 
  - 参数调优复杂
  - 可能过度去重
- **适用**: 高深度测序数据

**3. SeqKit**
- **原理**: 基于序列哈希
- **优点**: 
  - 处理速度极快
  - 内存使用低
  - 支持多种输入格式
- **缺点**: 
  - 主要用于单端数据
  - 双端配对处理较弱
- **适用**: 单端数据或预处理后的数据

#### 方法选择建议

**根据数据类型选择方法**:
```json
{
  "双端高质量数据": "fastuniq",
  "高深度数据": "clumpify", 
  "单端数据": "seqkit",
  "内存受限环境": "seqkit"
}
```

**根据重复率选择策略**:
- **低重复率 (<10%)**: 使用 FastUniq，快速处理
- **中等重复率 (10-30%)**: 使用 Clumpify，平衡效果和速度
- **高重复率 (>30%)**: 使用 Clumpify 的严格模式

#### 输出文件

| 文件名 | 内容 | 统计信息 |
|--------|------|----------|
| `*_R1_dedup.fastq.gz` | 去重复后的 R1 数据 | 主要输出 |
| `*_R2_dedup.fastq.gz` | 去重复后的 R2 数据 | 主要输出 |
| `*_dedup_summary.txt` | 处理统计报告 | 重复率、保留率等 |

#### 重复率解读

**正常重复率范围**:
- **基因组 DNA**: 5-15%
- **转录组 RNA**: 10-30%
- **ChIP-seq**: 15-40%
- **ATAC-seq**: 20-50%

**异常重复率**:
- **>50%**: 可能的 PCR 过度扩增或文库质量问题
- **<5%**: 可能的测序深度不足或样本质量极高

---
## 📊 步骤 5：MultiQC 综合报告

### 任务：`RunMultiQC`

MultiQC 整合所有质量控制工具的输出，生成统一的 HTML 报告。

#### 工作流调用

```wdl
Array[File] all_qc_reports = flatten([
    [InitialQC_R1.fastqc_html, InitialQC_R1.fastqc_zip],
    select_all([InitialQC_R2.fastqc_html, InitialQC_R2.fastqc_zip]),
    [FinalQC_R1.fastqc_html, FinalQC_R1.fastqc_zip],
    select_all([FinalQC_R2.fastqc_html, FinalQC_R2.fastqc_zip]),
    select_all([FastPPE.fastp_html, FastPPE.fastp_json, FastPSE.fastp_html, FastPSE.fastp_json])
])

call MultiQC.RunMultiQC as GenerateReport {
    input:
        input_files = all_qc_reports,
        sample_name = sample_name,
        output_name = sample_name + "_preprocessing_report"
}
```

#### 输入参数详解

| 参数名 | 类型 | 默认值 | 说明 | 用途 |
|--------|------|--------|------|------|
| `input_files` | Array[File] | 必需 | 所有 QC 工具输出 | 报告整合的数据源 |
| `sample_name` | String | 必需 | 样本名称 | 报告标题和标识 |
| `output_name` | String | 必需 | 输出报告名称 | 输出文件命名 |
| `memory_gb` | Int | 8 | 内存需求 | 资源分配 |

#### 报告模块

MultiQC 自动检测并整合以下模块：

**1. FastQC 模块**
- 原始数据质量评估
- 最终数据质量评估
- 质量改善对比

**2. Trimmomatic 模块**
- 接头去除统计
- 质量过滤效果
- 读段保留率

**3. FastP 模块**
- 高级过滤统计
- 长度分布变化
- 质量分数改善

**4. 自定义统计**
- 去重复处理效果
- 整体数据流失统计
- 处理步骤总结

#### 配置选项

**样本名称清理**:
```yaml
sample_names_rename:
    - ["_R1_initial", " (R1 原始)"]
    - ["_R2_initial", " (R2 原始)"]
    - ["_R1_final", " (R1 最终)"]
    - ["_R2_final", " (R2 最终)"]
```

**模块显示顺序**:
```yaml
module_order:
    - fastqc:
        name: "FastQC (原始数据)"
    - trimmomatic
    - fastp
    - fastqc:
        name: "FastQC (最终数据)"
```

#### 输出文件

| 文件名 | 格式 | 内容 | 用途 |
|--------|------|------|------|
| `*_report.html` | HTML | 交互式综合报告 | 人工查看和分享 |
| `*_data.zip` | ZIP | 原始数据和图表 | 进一步分析和自定义 |
| `*_summary.txt` | TXT | 报告生成摘要 | 快速检查 |

---

# ⚙️ 资源配置和优化

## 计算资源配置

### 资源需求表

| 数据大小 | CPU 核心 | 内存 (GB) | 磁盘 (GB) | 预估时间 | 成本估算 |
|---------|---------|-----------|-----------|----------|----------|
| **小型** (<10GB) | 8-16 | 32-64 | 100-200 | 1-2 小时 | $5-15 |
| **中型** (10-50GB) | 16-32 | 64-128 | 200-500 | 2-4 小时 | $15-40 |
| **大型** (50-100GB) | 32-64 | 128-256 | 500-1000 | 4-8 小时 | $40-100 |
| **超大型** (>100GB) | 64+ | 256+ | 1000+ | 8+ 小时 | $100+ |

### 各步骤资源分配

| 处理步骤 | CPU 核心 | 内存 (GB) | 存储 (GB) | 预估时间 |
|---------|---------|-----------|-----------|----------|
| FastQC | 2-4 | 4-8 | 10 | 10-30 分钟 |
| Trimmomatic | 4-8 | 8-16 | 20 | 30-60 分钟 |
| fastp | 4-8 | 8-16 | 20 | 20-40 分钟 |
| Deduplication | 4-8 | 16-32 | 30 | 20-60 分钟 |
| MultiQC | 1-2 | 4-8 | 5 | 5-10 分钟 |

## 成本优化建议

### 1. 使用抢占式实例
```wdl
runtime {
    preemptible: 2    # 最多重试2次
    maxRetries: 1     # 失败后重试1次
}
```

### 2. 磁盘类型选择
- **SSD**: 高 I/O 密集型任务（FastQC, MultiQC）
- **HDD**: 存储密集型任务（大文件处理）

### 3. 内存优化
- 根据实际数据大小调整内存分配
- 避免过度分配导致的资源浪费

---

# 🎯 参数优化策略

## 根据数据质量调整

### 高质量数据 (Q30 > 90%)
```json
{
  "PreprocessingWorkflow.min_quality": 25,
  "PreprocessingWorkflow.min_length": 75,
  "PreprocessingWorkflow.RunFastPPE.complexity_threshold": 40,
  "PreprocessingWorkflow.RunTrimmomaticPE.memory_gb": 16
}
```

### 中等质量数据 (Q30 = 70-90%)
```json
{
  "PreprocessingWorkflow.min_quality": 20,
  "PreprocessingWorkflow.min_length": 50,
  "PreprocessingWorkflow.RunFastPPE.complexity_threshold": 30,
  "PreprocessingWorkflow.RunTrimmomaticPE.memory_gb": 16
}
```

### 低质量数据 (Q30 < 70%)
```json
{
  "PreprocessingWorkflow.min_quality": 15,
  "PreprocessingWorkflow.min_length": 30,
  "PreprocessingWorkflow.RunFastPPE.complexity_threshold": 20,
  "PreprocessingWorkflow.RunTrimmomaticPE.memory_gb": 32
}
```

## 根据应用场景调整

### 基因组重测序
- 保守的质量过滤
- 保留更多数据用于变异检测
- 重复率控制在 15% 以下

```json
{
  "PreprocessingWorkflow.min_quality": 20,
  "PreprocessingWorkflow.min_length": 50,
  "PreprocessingWorkflow.RemoveDuplicatesPE.method": "fastuniq"
}
```

### 转录组测序
- 适中的质量过滤
- 注意保留低表达基因的数据
- 重复率可接受 20-30%

```json
{
  "PreprocessingWorkflow.min_quality": 18,
  "PreprocessingWorkflow.min_length": 40,
  "PreprocessingWorkflow.RemoveDuplicatesPE.method": "clumpify"
}
```

### 目标区域测序
- 严格的质量过滤
- 最大化目标区域的覆盖质量
- 重复率控制在 10% 以下

```json
{
  "PreprocessingWorkflow.min_quality": 25,
  "PreprocessingWorkflow.min_length": 75,
  "PreprocessingWorkflow.RemoveDuplicatesPE.method": "fastuniq"
}
```

---

# 🔍 质量控制检查点

## 每个步骤的关键指标

### FastQC 检查
- [ ] 总序列数是否符合预期
- [ ] 质量分数分布是否正常
- [ ] GC 含量是否在合理范围
- [ ] 是否存在接头污染

### Trimmomatic 检查
- [ ] 配对读段保留率 >80%
- [ ] 接头去除是否彻底
- [ ] 质量过滤是否适度

### FastP 检查
- [ ] Q30 比例是否提升
- [ ] 长度分布是否合理
- [ ] 复杂度过滤是否适当

### 去重复检查
- [ ] 重复率是否在预期范围
- [ ] 数据损失是否可接受
- [ ] 配对关系是否保持

### MultiQC 检查
- [ ] 所有模块是否正常显示
- [ ] 质量改善趋势是否明显
- [ ] 异常样本是否需要重新处理

---
# 🛠️ 故障排除指南

## 常见问题和解决方案

### 1. 内存不足错误
**症状**: 
```
OutOfMemoryError: Java heap space
Task failed with exit code 137
```

**解决方案**:
- 增加 `memory_gb` 参数
- 减少 `threads` 参数
- 使用更大的实例类型
- 检查输入文件大小是否异常

### 2. 磁盘空间不足
**症状**:
```
No space left on device
Disk quota exceeded
```

**解决方案**:
- 增加 `disk_gb` 参数
- 清理中间文件
- 使用流式处理
- 检查临时文件累积

### 3. 质量过滤过于严格
**症状**: 数据损失 >50%

**解决方案**:
- 降低 `min_quality` 参数 (20 → 15)
- 减少 `min_length` 参数 (50 → 30)
- 调整复杂度阈值 (30 → 20)
- 检查原始数据质量

### 4. 去重复效果不佳
**症状**: 重复率仍然很高

**解决方案**:
- 更换去重复方法 (fastuniq → clumpify)
- 检查上游处理质量
- 考虑样本本身的特性
- 调整去重复参数

### 5. 工作流执行超时
**症状**: 任务运行时间过长

**解决方案**:
- 增加 `threads` 参数
- 优化资源分配
- 检查网络连接
- 分批处理大文件

### 6. 文件格式错误
**症状**: 
```
Invalid FASTQ format
Unexpected file format
```

**解决方案**:
- 验证输入文件格式
- 检查文件完整性
- 确认压缩格式正确
- 重新下载损坏的文件

---

# 📊 性能监控和调优

## 监控指标

### 系统资源监控
- **CPU 使用率**: 应保持在 80-90%
- **内存使用率**: 避免超过 90%
- **磁盘 I/O**: 监控读写速度
- **网络带宽**: S3 传输速度

### 任务执行监控
- **任务完成时间**: 与预期时间对比
- **数据保留率**: 每步骤的数据损失
- **质量改善程度**: Q30 比例变化
- **错误率**: 失败任务比例

## 性能调优建议

### 1. 并行度优化
```json
{
  "小文件 (<5GB)": {"threads": 4},
  "中等文件 (5-20GB)": {"threads": 8},
  "大文件 (>20GB)": {"threads": 16}
}
```

### 2. 内存分配优化
```json
{
  "FastQC": "数据大小 × 0.5",
  "Trimmomatic": "数据大小 × 1.0", 
  "FastP": "数据大小 × 0.8",
  "Deduplication": "数据大小 × 1.5"
}
```

### 3. 磁盘空间规划
```json
{
  "临时空间": "输入数据大小 × 3",
  "输出空间": "输入数据大小 × 1.5",
  "缓冲空间": "总需求 × 0.2"
}
```

---

# 📝 配置示例

## 完整配置示例

### 双端测序高质量数据
```json
{
  "PreprocessingWorkflow.sample_name": "SRR16760538",
  "PreprocessingWorkflow.fastq_r1": "s3://catface996-genomic/sequencing-data/SRR16760538_1.fastq.gz",
  "PreprocessingWorkflow.fastq_r2": "s3://catface996-genomic/sequencing-data/SRR16760538_2.fastq.gz",
  "PreprocessingWorkflow.reference_genome": "s3://catface996-genomic/reference-genome/GCF_002263795.1_ARS-UCD1.2_genomic.fna",
  "PreprocessingWorkflow.min_length": 75,
  "PreprocessingWorkflow.min_quality": 25,
  "PreprocessingWorkflow.threads": 16,
  "PreprocessingWorkflow.paired_end": true,
  
  "PreprocessingWorkflow.RunFastQC.memory_gb": 16,
  "PreprocessingWorkflow.RunTrimmomaticPE.memory_gb": 32,
  "PreprocessingWorkflow.RunFastPPE.memory_gb": 32,
  "PreprocessingWorkflow.RunFastPPE.max_length": 400,
  "PreprocessingWorkflow.RunFastPPE.complexity_threshold": 40,
  "PreprocessingWorkflow.RunFastPPE.enable_polyg_trimming": true,
  "PreprocessingWorkflow.RunFastPPE.enable_polyx_trimming": true,
  "PreprocessingWorkflow.RemoveDuplicatesPE.memory_gb": 32,
  "PreprocessingWorkflow.RemoveDuplicatesPE.method": "fastuniq",
  "PreprocessingWorkflow.RunMultiQC.memory_gb": 16
}
```

### 低质量数据宽松过滤
```json
{
  "PreprocessingWorkflow.sample_name": "low_quality_sample",
  "PreprocessingWorkflow.fastq_r1": "s3://bucket/low_quality_R1.fastq.gz",
  "PreprocessingWorkflow.fastq_r2": "s3://bucket/low_quality_R2.fastq.gz",
  "PreprocessingWorkflow.min_length": 30,
  "PreprocessingWorkflow.min_quality": 15,
  "PreprocessingWorkflow.threads": 8,
  
  "PreprocessingWorkflow.RunFastPPE.complexity_threshold": 20,
  "PreprocessingWorkflow.RunFastPPE.enable_polyg_trimming": false,
  "PreprocessingWorkflow.RemoveDuplicatesPE.method": "clumpify"
}
```

### 内存受限环境配置
```json
{
  "PreprocessingWorkflow.sample_name": "memory_limited",
  "PreprocessingWorkflow.threads": 4,
  
  "PreprocessingWorkflow.RunFastQC.memory_gb": 8,
  "PreprocessingWorkflow.RunTrimmomaticPE.memory_gb": 16,
  "PreprocessingWorkflow.RunFastPPE.memory_gb": 16,
  "PreprocessingWorkflow.RemoveDuplicatesPE.memory_gb": 16,
  "PreprocessingWorkflow.RemoveDuplicatesPE.method": "seqkit",
  "PreprocessingWorkflow.RunMultiQC.memory_gb": 8
}
```

---

# 📚 参考资源

## 工具文档
- [FastQC 用户手册](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)
- [Trimmomatic 文档](http://www.usadellab.org/cms/?page=trimmomatic)
- [FastP GitHub](https://github.com/OpenGene/fastp)
- [MultiQC 文档](https://multiqc.info/)
- [AWS Omics 用户指南](https://docs.aws.amazon.com/omics/)

## 最佳实践
- [GATK 最佳实践](https://gatk.broadinstitute.org/hc/en-us/sections/360007226651)
- [nf-core 流水线](https://nf-co.re/)
- [Illumina 数据处理指南](https://www.illumina.com/)

## 质量标准
- [ENCODE 数据标准](https://www.encodeproject.org/)
- [1000 Genomes 质量控制](https://www.internationalgenome.org/)

## WDL 语法参考
- [WDL 规范](https://github.com/openwdl/wdl)
- [Cromwell 文档](https://cromwell.readthedocs.io/)
- [Terra 平台指南](https://terra.bio/)

---

# 📞 技术支持

## 获取帮助

### 文档和教程
- 查看工作流文档和示例
- 参考 AWS Omics 用户指南
- 浏览生物信息学社区论坛

### 常见问题
- 检查参数配置是否正确
- 验证输入文件格式和路径
- 确认 AWS 权限设置

### 联系支持
- AWS 技术支持
- 生物信息学社区
- 项目维护团队

---

## 📋 文档信息

**文档标题**: 预处理工作流完整指南  
**版本**: 1.0  
**作者**: AWS Omics Demo Project  
**最后更新**: 2025-08-09  
**适用版本**: WDL 1.0, AWS Omics  

**更新历史**:
- v1.0 (2025-08-09): 初始版本，包含完整的工作流说明和参数详解

---

*本文档提供了 AWS Omics 预处理工作流的完整指南，包括详细的参数说明、优化建议和故障排除方法。如有疑问或建议，请参考相关文档或联系技术支持。*
